{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(base_path, subdir, out_path, file_type):\n",
    "    dir_path = os.path.join(base_path, subdir)\n",
    "    all_files = sorted(os.listdir(dir_path))\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        #print(file) # debugging\n",
    "        if file_type == 'te': # for train equipment\n",
    "            narr_columns = [f'NARR{i}' for i in range(1, 16)]\n",
    "            cols = ['YEAR', 'MONTH', 'DAY', 'TYPE', 'CARSDMG', \n",
    "                    'CARSHZD', 'STATE', 'TEMP', 'VISIBLTY', 'WEATHER',\n",
    "                    'TRNSPD', 'TONS', 'TYPEQ', 'TYPTRK', 'CAUSE', \n",
    "                    'CASKLDRR', 'CASINJRR', 'ACCDMG', 'Latitude', \n",
    "                    'Longitud'] + narr_columns\n",
    "            df = pd.read_csv(file_path, index_col=False, dtype=str) # csv\n",
    "            df = df[cols]\n",
    "            merged_df = pd.concat([merged_df, df], axis=0)\n",
    "            #print(merged_df['YEAR'].unique()) # debugging\n",
    "        elif file_type == 'app': # for appendix\n",
    "                df = pd.read_excel(file_path, sheet_name='Sheet1', dtype=str) #xls\n",
    "                df.columns = df.columns.str.upper()\n",
    "                cols = ['CODE', 'DESCRIPTION', 'CATEGORY', 'TITLE']\n",
    "                df = df[cols]\n",
    "                merged_df = pd.concat([merged_df, df], axis=0)\n",
    "        else:\n",
    "            print(f\"Invalid file type: {file_type}\")\n",
    "            \n",
    "    output_file = os.path.join(out_path, f'{subdir}_merged.csv')\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "subdirectories = ['TrainEquipment', 'TrainEquipmentAppendix']\n",
    "\n",
    "for subdir in subdirectories:\n",
    "    file_type = \"te\" if subdir=='TrainEquipment' else \"app\"\n",
    "    #print(file_type) # debugging\n",
    "    merge_csv(path, subdir, path, file_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://railroads.dot.gov/forms-guides-publications/guides/618054-rail-equipment-accidentincident-thru-52011-206kb\n",
    "# https://www.bls.gov/respondents/mwr/electronic-data-interchange/appendix-d-usps-state-abbreviations-and-fips-codes.htm\n",
    "def convert_to_date(year):\n",
    "    year = int(year)\n",
    "    if 75 <= year <= 99:\n",
    "        return 1900 + year\n",
    "    elif 0 <= year <= 22:\n",
    "        return 2000 + year\n",
    "    return None\n",
    "\n",
    "def convert_type(type):\n",
    "    types = {\n",
    "        1: \"Derailment\",\n",
    "        2: \"Head on Collision\",\n",
    "        3: \"Rearend Collision\",\n",
    "        4: \"Side Collision\",\n",
    "        5: \"Raking Collision\",\n",
    "        6: \"Broken Train Collision\",\n",
    "        7: \"Hwy-Rail Crossing\",\n",
    "        8: \"RR Grade Crossing\",\n",
    "        9: \"Obstruction\",\n",
    "        10: \"Explosive Detonation\",\n",
    "        11: \"Fire / Violent Rupture\",\n",
    "        12: \"Other Impacts\",\n",
    "        13: \"Other\"\n",
    "    }\n",
    "    type = int(type)\n",
    "    if type in types:\n",
    "        return types[type]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def convert_state(fips_code):\n",
    "    fips_to_state = {\n",
    "        '01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA', '08': 'CO', '09': 'CT',\n",
    "        '10': 'DE', '11': 'DC', '12': 'FL', '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL',\n",
    "        '18': 'IN', '19': 'IA', '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME', '24': 'MD',\n",
    "        '25': 'MA', '26': 'MI', '27': 'MN', '28': 'MS', '29': 'MO', '30': 'MT', '31': 'NE',\n",
    "        '32': 'NV', '33': 'NH', '34': 'NJ', '35': 'NM', '36': 'NY', '37': 'NC', '38': 'ND',\n",
    "        '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI', '45': 'SC', '46': 'SD',\n",
    "        '47': 'TN', '48': 'TX', '49': 'UT', '50': 'VT', '51': 'VA', '53': 'WA', '54': 'WV',\n",
    "        '55': 'WI', '56': 'WY'\n",
    "    }\n",
    "    \n",
    "    if fips_code in fips_to_state:\n",
    "        return fips_to_state[fips_code]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def convert_vis(vis):\n",
    "    vis_types = {\n",
    "        '1': \"Dawn\",\n",
    "        '2': \"Day\",\n",
    "        '3': \"Dusk\",\n",
    "        '4': \"Dark\"\n",
    "    }\n",
    "    if pd.isnull(vis):\n",
    "        return np.nan\n",
    "    elif str(vis) in vis_types:\n",
    "        return vis_types[str(vis)]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def convert_weather(weather):\n",
    "    weather_types = {\n",
    "        '1': \"Clear\",\n",
    "        '2': \"Cloudy\",\n",
    "        '3': \"Rain\",\n",
    "        '4': \"Fog\",\n",
    "        '5': \"Sleet\",\n",
    "        '6': \"Snow\"\n",
    "    }\n",
    "    if pd.isnull(weather):\n",
    "        return np.nan\n",
    "    elif str(weather) in weather_types:\n",
    "        return weather_types[str(weather)]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def convert_eq(eq):\n",
    "    eq_types = {\n",
    "        \"1\": \"Freight Train\",\n",
    "        \"2\": \"Passenger Train\",\n",
    "        \"3\": \"Commuter Train\",\n",
    "        \"4\": \"Work Train\",\n",
    "        \"5\": \"Single Car\",\n",
    "        \"6\": \"Cut of Cars\",\n",
    "        \"7\": \"Yard / Switching\", # at a yard or while switching\n",
    "        \"8\": \"Light Loco\",\n",
    "        \"9\": \"Maint / Inspect Car\", \n",
    "        \"A\": \"Spec. MoW Eq.\" # Maintenance-of-Way \n",
    "    }\n",
    "    if eq in eq_types:\n",
    "        return eq_types[eq]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def convert_track(track):\n",
    "    track_types = {\n",
    "        '1': \"Main\",\n",
    "        '2': \"Yard\",\n",
    "        '3': \"Siding\",\n",
    "        '4': \"Industry\"\n",
    "    }\n",
    "    if pd.isnull(track):\n",
    "        return np.nan\n",
    "    elif str(track) in track_types:\n",
    "        return track_types[str(track)]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = pd.read_csv('./data/TrainEquipment_merged.csv', dtype=str)\n",
    "te.dropna(subset=['VISIBLTY', 'WEATHER', 'TYPTRK', 'TRNSPD'], inplace=True) # drop NaNs\n",
    "\n",
    "## merge YEAR, MONTH, DAY cols to a DATE col\n",
    "te = te[te['DAY'] != '00'] # filter out invalid rows where day == 00\n",
    "te['YEAR'] = te['YEAR'].apply(convert_to_date)\n",
    "te['DATE'] = pd.to_datetime(te[['YEAR', 'MONTH', 'DAY']], format='%Y-%m-%d')\n",
    "te.drop(columns=['YEAR', 'MONTH', 'DAY'], inplace=True) # drop YEAR, MONTH, DAY cols\n",
    "te = te.sort_values(by='DATE')\n",
    "\n",
    "## convert encoding to informative values based on documentation\n",
    "te['TYPE'] = te['TYPE'].apply(convert_type)\n",
    "te['STATE'] = te['STATE'].apply(convert_state)\n",
    "te['VISIBLTY'] = te['VISIBLTY'].apply(convert_vis)\n",
    "te['WEATHER'] = te['WEATHER'].apply(convert_weather)\n",
    "te['TYPEQ'] = te['TYPEQ'].apply(convert_eq)\n",
    "te['TYPTRK'] = te['TYPTRK'].apply(convert_track)\n",
    "\n",
    "## convert numeric cols to int\n",
    "te['CARSDMG'] = pd.to_numeric(te['CARSDMG'], errors='coerce').astype(int)\n",
    "te['CARSHZD'] = pd.to_numeric(te['CARSHZD'], errors='coerce').astype(int)\n",
    "te['TEMP'] = pd.to_numeric(te['TEMP'], errors='coerce').astype(int)\n",
    "te['TRNSPD'] = pd.to_numeric(te['TRNSPD'], errors='coerce').astype(int)\n",
    "te['TONS'] = pd.to_numeric(te['TONS'], errors='coerce').astype(int)\n",
    "te['CASKLDRR'] = pd.to_numeric(te['CASKLDRR'], errors='coerce').astype(int)\n",
    "te['CASINJRR'] = pd.to_numeric(te['CASINJRR'], errors='coerce').astype(int)\n",
    "te['ACCDMG'] = pd.to_numeric(te['ACCDMG'], errors='coerce').astype(int)\n",
    "\n",
    "te.dropna(subset=['VISIBLTY', 'WEATHER', 'TYPTRK', 'TRNSPD'], inplace=True) # drop NaNs resulting from conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_app = pd.read_csv('./data/TrainEquipmentAppendix_merged.csv', dtype=str)\n",
    "\n",
    "te_app['CODE'] = te_app['CODE'].str.strip()\n",
    "\n",
    "## merge based on accident code\n",
    "merged_df = pd.merge(te, te_app, left_on='CAUSE', right_on='CODE', how='inner')\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['DATE', 'TYPE', 'STATE', 'CAUSE', 'TITLE', 'CATEGORY', 'DESCRIPTION', \n",
    "        'CASKLDRR', 'CASINJRR', 'CARSDMG', 'CARSHZD', 'TEMP', 'VISIBLTY', 'WEATHER', \n",
    "        'TRNSPD', 'TONS', 'TYPEQ', 'TYPTRK', 'ACCDMG', 'NARR1', 'NARR2', 'NARR3', \n",
    "        'NARR4', 'NARR5', 'NARR6', 'NARR7', 'NARR8', 'NARR9', 'NARR10', 'NARR11', \n",
    "        'NARR12', 'NARR13', 'NARR14', 'NARR15', 'Latitude', 'Longitud']\n",
    "merged_df = merged_df.reindex(columns=cols)\n",
    "merged_df.to_csv('./data/train_final.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airplanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = pd.read_csv('./data/airplanes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change datetime format to match train data\n",
    "ap['Date'] = pd.to_datetime(ap['Date'], format='%m/%d/%Y')\n",
    "ap['Date'] = ap['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "ap.drop(columns=['Time', 'Flight #', 'Registration', 'Route', 'cn/In'], inplace=True) # drop unneeded cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://stackoverflow.com/questions/1396084/regex-for-comma-delimited-list\n",
    "# https://www.rexegg.com/regex-quickstart.html\n",
    "states = (\n",
    "    \"Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|\"\n",
    "    \"Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|\"\n",
    "    \"Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|\"\n",
    "    \"New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|Ohio|Oklahoma|\"\n",
    "    \"Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|\"\n",
    "    \"Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming\"\n",
    ")\n",
    "\n",
    "pattern = r\",\\s*(\" + states + r\")$\" # U.S. incidents are in format city, state\n",
    "\n",
    "state_names = ap['Location'].str.extract(pattern, expand=False) # extract state names from all rows; rows that don't contain a state are NaN\n",
    "mask = state_names.notna() # remove rows that don't contain a state\n",
    "ap = ap[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_abbreviation(state):\n",
    "    state_abbr = {\n",
    "        'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "        'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "        'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n",
    "        'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "        'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n",
    "        'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n",
    "        'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA',\n",
    "        'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n",
    "    }\n",
    "    return state_abbr.get(state)\n",
    "\n",
    "\n",
    "ap[['City', 'State']] = ap['Location'].str.extract(r'(.*),\\s*(' + states + r')$', expand=True) # separate city and state into separate cols\n",
    "ap['State'] = ap['State'].apply(state_abbreviation) # change state col into abbreviated encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Date', 'Location', 'City', 'State', 'Operator', 'Aboard', 'Fatalities', 'Ground', 'Type','Summary']\n",
    "ap.to_csv('./data/airplanes_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anly503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
